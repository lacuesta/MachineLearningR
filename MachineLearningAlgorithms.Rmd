---
title: "Machine Learning Methods with R"
author: "Vicente Lacuesta"
date: "20 de octubre de 2015"
output: html_document
---

# Nearest neighbors algorithm
Pros:
- Simple and effective
- No assumptions about the underliying data distribution
- Fast training phase

Cons:
- Does not produce a model
- Slow classification phase
- Requires a large amount of memory
- Nominal features and missing data require additional processing

## kNN algorithm
The kNN algorithm uses a dataset composed of examples classified
in several categories, as labeled by a nominal variable. kNN identifies k
records that are the nearest in similarity.
The features should be transformed to a standard range before applying the kNN algorithm, as min-max normalization or z-score standarization.

### Building the classifier and making predictions
Function `knn()` in the `class` package,
```{r}
p <- knn(train, test, class, k)
```
- train: data frame containing numeric training data
- test: data frame containing numeric test data
- class: factor vector with the class for each row in the training data
- k: integer indcating the number of nearest neighbors


# Naive Bayes algorithm
Pros:
- Simple, fast and effective
- Does well with noisy and missing data
- Requires few examples for training
- Easy to obtain the estimated probability for prediction

Cons:
- Relies on the assumption of equally important and independent features
- Not ideal for datasets with large numbers of numeric features
- Estimated probabilities are less reliable than the predicted classes

It uses frequency tables, so each feature must be categorical in order to create the combinations of class and feature values. The solutions are: discretize (binning).

## Naive Bayes classification syntax.
Use the `naiveBayes()Â´ function in the `e1071` package.

### Build the classifier
```{r}
m <- naiveBayes(train, class, laplace = 0)
```
- train: data frame or matrix containing the training data
- class: factor vector with the class for each row in the training data
- laplace: number to control the Laplace estimator

### Make predictions
```{r}
p <- predict(m , test, type = "class")
```
- m: model trained by `naiveBayes()`
- test: data frame or matrix with test data with the same features as the training data
- type: "class" or "raw" that specifies if the predictions should be the most likely class value or the raw predicted probabilities.

# Decision trees

## The C5.0 decision tree algorithm
Pros:
- All purpose classifier
- Highly automatic learning process that can handle numeric or nominal features and missing data
- Uses the most importand features
- Can be used on data with relatively few training examples
- Results in a model easy to interpret
- Very efficient

Cons:
- Often biased toward splits on features having a large number of levels
- Easy to overfit or underfit
- Trouble modeing some relationships due to reliance on axis-parallel splits
- Small changes in training data can result in large changes to the decision tree
- Large trees difficult to interpret

### C5.0 decision tree syntax
Use the `C5.0()` function in the `C50` package

### Build the model
```{r}
m <- C5.0(train, class, trials = 1, costs = NULL)
```
- train: data frame with the training data
- class: factor vector with the class for each row in the training data
- trials: optional number to control the number of boosting iterations.
- costs: optional matrix specifyig costs associated with types of errors

### Make predictions
```{r}
p <- predict(m, test, type = "class")
```
- m: model trained by the C5.0() function
- test: data frame containing test data
- type: "class" or "prob", specifies wheter the predictions should be the most likely class value or the raw predicted probabilities


## One Rule algorithm
Selects a single rule, this may seem simplistic but it tend to perform better than expected.
Pros:
- Generates a single rule
- Obten performs very well
- Can serve as benchmark for more complex algorithms

Cons:
- Uses only a single feature
- Very simplistic

### 1R classification rule syntax
Use the `OneR()` function in the `RWeka` package

### Build the classifier:
```{r}
m <- oneR(class ~ predictors, data = mydata)
```
- class: column in mydata data frame to be predicted
- predictors: R formula specifying the features in the mydata data frame to use for predictions
- data: data frame in which class and predictors can be found

### Make predictions:
```{r}
p <- predict(m, test)
```
- m: model trained by the OneR() function
- test: data frame containif test data


## Ripper algorithm
Repeated incremental oruning to produce error reduction
Pros:
- Generates easy rules
- Efficient on large and noisy datasets
- Produces a simpler model than a comparable decision tree

Cons:
- May result in rules that seem to defy common sense
- Not ideal for numeric data
- Might not perform as well as more complex models

### Ripper classification rule syntax
Use the `JRip()` function in the `RWeka` package

### Build the classifier:
```{r}
m <- JRip(class ~ predictors, data = mydata)
```
 class: column in mydata data frame to be predicted
- predictors: R formula specifying the features in the mydata data frame to use for predictions
- data: data frame in which class and predictors can be found

### Make predictions:
```{r}
p <- predict(m, test)
```
- m: model trained by the JRip() function
- test: data frame containif test data

# Regression Methods

## Multiple linear regressions
Pros:
- The most common approach for modeling numeric data.
- Can be adapted to model almost any data
- Provides estimates of the strength and size of the relationships moing features and the outcome

Cons:
- Makes strong assumptions about the data
- The model's form must be specified in advance
- Does not perform well with missing data
- Only works with numeric features
- Requires some knowledge of statistics

### Multiple regression modeling syntax
Use the `lm()` function in the `stats` package.

### Build the model
```{r}
m <- lm(dv ~ iv, data = mydata)
```
- dv: dependent variable in the mydat data frame to be modeled
- iv: R formula specifying the independent variables in mydata data frame
- data: the data frame

### Make predictions
```{r}
p <- predict(m, test)
```
- m: model trained by the `lm()` function
- test: data frame containing test data

## Regression trees
Pros:
- Combines the strenghs of decision trees with the ability to model numeric data
- Does automatic feature selection
- Does not require to specify the model in advance
- May fit some types of data better than linear regression
- Does not require knowledge of statistics

Cons:
- Not as commonly-used as linear regression
- Requires a large amount of training data
- Difficult to determine the overall net effect of individual features
- May be more difficult to interpret

### Regression trees syntax
Use `rpart()` function in the `rpart` package

### Build the model
```{r}
m <- rpart(dv ~ iv, data = mydata)
```
- dv: dependent variable in the mydat data frame to be modeled
- iv: R formula specifying the independent variables in mydata data frame
- data: the data frame

### Make predictions
```{r}
p <- predict(m, test, type = "vector")
```
- m: model trained by the `rpart()` function
- test: data frame containing test data
- type: type of prediction, "vector" (predicted numeric values), "class" (predicted classes) or "prob" (predicted class probabilities)


## Model tree
Replace the leaf nodes with regression models.

### Model trees syntax
Use the `M5P()` function in the `RWeka` package

### Build the model
```{r}
m <- M5P(dv ~ iv, data = mydata)
```
- dv: dependent variable in the mydat data frame to be modeled
- iv: R formula specifying the independent variables in mydata data frame
- data: the data frame

### Make predictions
```{r}
p <- predict(m, test)
```
- m: model trained by the `M5P()` function
- test: data frame containing test data

# Neural Networks 
Pros:
- Can be adapted to classification or numeric prediction problems
- Among the most accurate modeling approaches
- Makes few assumptions about the data's underlying relationships
Cons:
- Computiationally intensive and slow to train
- Easy to overfit or unferfit
- Results in a complex black box model.

### Neural network syntax
Use the `neuralnet()' function in the 'neuralnet' package.

### Build the model
```{r}
m <- neuralnet(target ~ predictors, data = mydata, hidden = 1)
```
- target: outcome in the mydata data frame to be modeled
- predictors: R formula specifying the features in the mydata data frame
- hidden: number of neurons in the hidden layer

### Make predictions
```{r}
p <- compute(m, test)
```
- m: model trained by the `neuralnet()`function
- test: data frame containing test data with the same features as the training data
The function returns a list with two components: `$neurons` which stores the neurons for each layer in the network and `$net.result` which stores the model's predicted values

# Support Vector Machines
Pros:
- Can ve used for classification or numeric prediction problems
- Not influenced by noisi data
- Easier than neural networks
- High accuracy

Cons:
- Finding the best model requires testing of various combinations of kernel and model parameters
- Can be slow to train
- Results in complex black bo model
## Support vector machine syntax
Use the `ksvm()` function in the `kernlab` package.

## Build the model
```{r}
m <- ksvm(target ~ predictors, data = mydata, kernel = "rbfdot", C = 1)
```
- target: the outcome in the mydata data frame to be modeled
- predictors: R formula specifying the features to use for prediction
- data: the training data frame
- kernel: nonlinear mapping such as "rbfdot" (radial basis), "polydot"(polynomial), "tanhdot"(hyperbolic tangentsigmoid) or "vanilladot" (linear)
- C: number that specifies the cost of violating the constraints.

## Make predictions
```{r}
p <- predict(m, test, type = "response")
```
- m: model trained by the `ksvm()`function
- test: data frame with the test data
- type: "response" (predicted class) or "probabilities" (predicted probabolity)

# Association rules
Pros: 
- Suited for working with very large amounts of transactional data
- Rules easy to understand
- Useful for data mining

Cons:
- Not helpfull for small datasets
- Takes effort to separate the insight from the common sense
- Easy to draw spurious conclusions from random patterns

## Association rule syntax
Use `apriori()` function in `arules` package

## Find association rules:
```{r}
myrules <- apriori(data = mydata, parameter = list(support = 0.1,
                                                   confidence = 0.8,
                                                   minlen = 1))
```
- data: sparse item matrix holding transactional data
- support: minimum required rule support
- confidence: minimum required rule confidence
- minlen: minimum required rule items

## Examine association rules
```{r}
inspect(myrules)
```

# Clustering with k-means

## k-means algorithm for clustering
Pros:
- Uses simple principles for identifying clusters
- highly flexible
- Fairly efficient 

Cons:
- Low sophisticated
- Uses elements of random chance
- Requires a reasonable guess as to how many clusters exist in the data

### Clustering syntax
Use the `kmeans()Â´function in the Â´statsÂ´ package

### Find clusters:
```{r}
myclusters <- kmeans(mydata, k)
```
- mydata: matrix or data frame with the examples to be clustered
- k: desired number of clusters

### Examine clusters
- mycluster$cluster: vector of cluster assignments
- mycluster$centers: matrix indicating the men values for each features
- myclusters$size: number of examples assigned to each cluster

# Random forests
Pros: 
- All-purpose model that perfoms well on most problems
- Can handle noisy or missing data, categorical or continous features
- Selects the most important features
- Can be used on data with high number of features or examples

Cons:
- Not easily interpretable
- May require some work to tune the model

## Random forest syntax
Use `randomForest()` function in the `randomForest` package

## Build the classifier
```{r}
m <- randomForest(target ~ predictors, data = mydata, ntree = 500, mtry = sqrt(p))
```
- target: outcome in the mydata data frame to be modeled
- predictors: R formula specifying the features in the mydata data frame
- data: data frame with training data
- ntree: number of trees to grow
- mtry: optional integer specifying the number of features to randomly select at each split, p is the number of features in the data

## Make predictions
```{r}
p <- predict(n ,test, type = "response")
```
- m: model trained by the `randomForest()` function
- test: data frame with the test data
- type: "response" (predicted class), "prob" (predicted probabilities) or "votes"(matrix of vote counts).

